{
  "text": "Hello, everyone, and welcome back to Conversations with Tyler. Today, I am at Anthropic with Jack Clark. As you may know, there is now an Anthropic economic index which is measuring the effect of advanced AI on the US economy. There are two associated reports as of March 2025 and soon more to come. Jack, of course, is the co-founder of Anthropic. Before that was the policy director at OpenAI, a reporter at Bloomberg, and originally has his background in the humanities, and he comes from Brighton, England. Jack, welcome. Well, thanks for having me, Tyler. Pleasure to be here. Where is it in our economy that AGI will affect last in a significant manner? Ooh, I'd hazard a guess that it's going to be things that are the trades and the most artisanal parts of them. So you might think of trades as having things like electricians or plumbing or also things like gardening, but I think within those, you get certain high-status, high-skill parts where people want to use a certain tradesman not just because of their skill, but because of their notoriety and sometimes an aesthetic quality. I think that my take might be gardening, actually. And they won't use AGI to help design the garden, or just the human front will never disappear? I think the human front will never disappear, and people will purchase certain things because of the taste of the person. Even if that taste looks like certain types of modern art production where the artist actually backs on to thousands of people that work for them, and they're more orchestrating it. And how about in the more desk-bound part of the service sector? Where will it come last? Come last? Ooh, good question. I think that on this, there are certain types of desk-bound work that just require talking to other people and getting to alignment or agreement. You might, if you count certain types of sales... But it's great at doing that already, right? It's a wonderful therapist. It is, but we don't send Claude to sell Claude yet. We send people to sell Claude, even though Claude could probably generate the text to do the sales motion. People want to do commerce with other people. So I think that there'll be certain relationships which get mediated by people. And people will have a strong preference, probably for deals that they make on behalf of their larger pools of capital, where the deals are done by human proxies for large automated organizations or pools of capital. Where will AGI encounter the strongest legal obstacles? I think a few years ago, it was encountering quite strong obstacles in the law itself because lawyers tended to like being able to charge very high prices and had a dislike of things that could bid them down. But my less flippant answer is probably big chunks of healthcare because it's bound up in certain things around how we handle personal data, all of the standards around that. All of those standards are probably going to need to be changed in some form to make them amenable to being used by AI. And we've had a really hard time updating or changing data standards in general. But once you can put the AI on your own hard drive, which will be pretty soon, right? Won't that all change? It will change in the form of gray market expertise, but not official expertise. I had a baby recently, and so whenever my baby, like, bonks their head, while I'm dialing the advice nurse, I talk to Claude just to reassure myself that the baby isn't in trouble. I don't think we actually fully permit healthcare uses via our own terms of service. We don't recommend it because we're worried about all of the liability issues this contains. But I know through my revealed preference that I'm always going to want to use that. But I can't take that Claude assessment and give it to Kaiser Permanente. I actually have to talk through a human to get everything else to happen on the back end to work out if they need to prescribe my child something. So the number one job will be surreptitiously transmitting the generation of information that comes from AIs, in essence? Some of it may be that. Some of it is about laundering the information that comes from AIs into human systems that are not predisposed to that information going in directly. I was thinking you might say that United States government or some parts of it would be where strong AI would come last. I think that would be my forecast. They still use software from the 60s or maybe even the 50s sometimes. They do, but I actually suspect that that could be a place where we see really rapid changes, actually. And for a couple of reasons. One, we know that AI has relevance to national security. It develops certain types of capabilities. Oh, yeah, that will happen quickly. That will happen quickly. But the rest of government. The rest of government. Department of Education, HHS. The non-scary, sharp part of government. Right. Yeah. I would wager that it will become surprisingly easy to get it into really hard parts of government and then there will be a question of political will, which is where the system might, this all might break down. What do you think is the chance that we decide to protect, say, half of the jobs in existence today with laws analogous to those we find in law and medicine against strong AGI? I think there is a high chance for a political movement to arrive which tries to freeze a load of human jobs in bureaucratic amber as a way to deal with the political issues posed by this incredibly powerful technology and how fast the changes are going to come from. I don't think that we'll do this in, like, a reasoned way. I think it'll be driven by the chaotic winds of, like, political forces. And it feels like the sort of outcome that happens if we, as the companies building this stuff and our customers, don't generate enough examples of what, like, good transitions look like. The fewer bits of evidence you have there and the more evidence you have of larger economic changes, probably the higher chance there'll be a desire to step in and protect workers in different domains, which comes from an impulse based around wanting to help people, but it might ultimately not be the most helpful thing over the course of decades to do that. But is it possible that's actually, in a constrained sense, the very best outcome? People will still have a job. They'll go somewhere in the morning. A lot of the work, especially the hard work, will be done by the AIs. Obviously, we'll be richer, so we can afford this all of a sudden. It won't feel that bad to people. Life will look familiar. Isn't that, in a sense, what we should be aiming for? I think we should be aiming for... In the same sense, you might need, say, a generous welfare state to have free trade. The welfare state isn't always the most efficient, but if people accept freer trade, it's an okay bargain. Isn't this, in a sense, the welfare state for the service workers, and they still get to go somewhere in the morning if they want to, but they don't have to? I believe that people, all people, have a desire for meaning and salience to what they do on a day-to-day basis. My worry with what you describe is it might not feel like it has meaning, sufficient meaning. I think that there is some giant class of activity that we want to continue happening in the world that people do and from which they draw meaning, but I don't know that the best way to get there is to take some class of work and say, this is work that we're protecting and from which meaning will spring, because I'm not confident that you'll pick a load of jobs that naturally create their own meaning in that sense. But hasn't this succeeded in academia, pre-AI? So most academic jobs are not that meaningful. The research people do, it's not read by anyone. Maybe they're decent teachers, but people take great pride in their research. They put a lot of effort into it. It's meaningless. It seems we solved that problem already. We're just going to take the academic model, but instead of it being research, bring it to, say, half of our current economy just to keep it going. But isn't there a kind of angst and nihilism even within high-achieving parts of academia for this reason? It seems like when you speak to people, even very smart people, who sometimes are doing the things you describe, they know they could be doing different things and they're trapped into some kind of status game. There's some of that. But even the Nobel laureates, they're rivalrous with each other. They can be very bitchy, very petty. But that's just human nature, right? If the Nobel laureates aren't happy, there's no post-AI world that's going to do much better than how we're doing for the Nobel laureates today, right? Yeah, maybe my pushback is I think that all of this could happen sufficiently quickly that we might have the opportunity to just play different higher-status games that are afforded to us via AI and the productive capacity it unlocks. I'm wondering if there are definitely going to be entirely new jobs that involve marshalling and fielding AI systems for all kinds of work. But those are hard jobs, right? Yes, but I think that there are going to be analogs which look more like creative, fun exercises in getting AIs to kind of build things or make things or almost carry out competitions and games where people can play them with one another. And I think there'll be entirely new forms of entertainment that has some amount of meaning and perhaps an economic engine wired into it that people can participate in. I believe we're not that far from the age of what I call the AI teddy bears. You know what I mean when I say that. What percentage of parents now will buy those teddy bears for their kids and allow it? I mean, I've had this thought since I have a person that is my child that's almost two."
}