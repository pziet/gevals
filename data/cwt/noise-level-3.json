{
  "text": "Hello everyone, and welcome back to the Mindscape Economics. Today I am at the Prophets in Jack Park. As you may know, there is now a Prophets Economic Index, which is measuring the effect of Advanced AI on the U.S. economy. There are two associated reports, as of March 2045. Jack is the co-founder of Prophets, before that he was the policy director of the Uber, and a reporter at Uber. Originally he was at the University of Humanity, and he comes from a great family. Where do you see in our economy the AGI effect? Ooh, I'd hazard a guess that it's going to be things that are the trades and the most artisanal parts of our world. So you might think of trades as having things like electricians or plumbing or something like gardening, but I think within those you get certain high-status, high-skilled parts where people want to use a certain tradesman not just because of their skill, but because of their notoriety, and sometimes an aesthetic quality. I think my take might be gardening. They won't use AGI to help with their garden, or just the human front of the work? I think the human front will never disappear, and people will purchase certain things because of their taste of the person, even if that taste looks like certain types of modern art production where the artist actually backs on to thousands of people that work for them, and they're more orchestrating. How about in the more desk-bound part of the service sector? Where will it come last? I think that one is... There are certain types of desk-bound work that just require talking to other people and getting into alignment. It might be council-type things. It is, but we don't send people to sell board, yet we send people to sell board. People go board and probably generate the text to do the sales motion. People want to do the commerce a lot with people. So I think certain relationships will get mediated by people, and people will have a strong preference, probably for deals that they make on behalf of their larger organizations. So the deals are done by human proxies for large automated organizations. How will AGI encounter the smug of the legal arm? I think a few years ago it was encountering quite strong opposition for itself because lawyers tended to like being able to charge very high prices and had a dislike of being able to bring them down. But my less-brief response is probably big chunks of healthcare because it's bound up in certain things around health and personal data, all of the standards around that. All of those standards are probably going to need to be changed in some form to make them amenable to being used by AI, and we've had a really hard time updating or changing data standards in the future. Once you can put the AI into my drive, what will that be? It will change and reform healthcare in the sense that it's not an inefficient system. I had a baby recently, and so when my baby coughs their head while I'm dialing the advice nurse, I cough at the board just to reassure myself that the baby is out of trouble. I don't think we actually use them in healthcare uses by our own terms. We don't recommend it because we're worried about all the liability issues it contains, but I know from my review of the data that that computer wants to use that. But I can't take that board assessment and give it to Kaiser Permanente. I actually have to go through a human to get everything else to happen on the back end to work out if they need to drive my child. So the number one job would be just me transmitting the generation of information that comes from AI in essence? Some of it is about laundering the information that comes from AI into human systems that are not pre-disposed to that information. I was thinking you might say that the United States government, for some reason, would be a strong AI company. I think that would be my point. They still use software from the 60s or even the 80s. They do, but I actually suspect that that could be the place where we see really rapid changes. And for a couple of reasons. One, we know that AI has relevance to national security. It develops certain types of capabilities. That will happen for the rest of the company. That will happen for the rest of the company. Not necessarily for the rest of the company. I would weigh on that it will become surprisingly easy to get it into really hard parts of government and then there will be threats to the political will. But all around the world, I'm sure you experience this, governments desperately offer a bit of money for efficiency and we see that here today. They say that. I think if you look at also things like voter polling and other things, people want to see more changes out of government than they're currently getting. And I think sometimes constituent preferences do ultimately change what their elected officials do. I would just take the other side of this, but government may move slightly faster than you think. It may be very large established companies that end up having some of the greatest benefits of this scenario. Let's say it was decided that half of the staff of the HSE or the Department of Education could be replaced by strong AI or AGI. Do we first need to hire more people if that happens? And who is it we can hire at what wage that switches the system so you can lay off the remaining staff? I don't understand how that's ever going to work. I think that it would only work in a scenario where the system felt so powerful that we can bring the AI system in itself to help them think through this. And then there will be a question of political will, which is where the system might break down. What do you think is the chance that we decide to protect, say, half of the jobs in the system? The law is analogous to the law. I think there is a high chance of a political movement to arrive which tries to freeze a load of human jobs in bureaucratic paper as a way to deal with the political issues posed by this incredibly powerful technology and how fast the changes are going to come. I don't think we will do this in a pleasing way. I think this will be driven by the chaotic winds of political forces and it feels like the sort of outcome that happens if we as companies building this stuff and our customers don't generate enough examples of what good transitions look like. The fewer bits of evidence we have there, the more evidence we have of larger economic changes. Probably the higher chance there will be a desire to step in and protect workers from different domains becomes worse. An impulse based around wanting to help young people but it might ultimately not be the most helpful thing over the course of decades. Is it possible that's actually the constraint set that's the very best outcome? People will still have jobs because we're more important. A lot of the work, especially the hard work, will be done by young people. Obviously we'll be richer. It won't be the bad people who might go looking for jobs. Is that what we should be aiming for? In the same sense you might need to have a generous welfare state in the country. The welfare state isn't always the most efficient model people accept. It's an okay bargain. Is this in a sense the welfare state that serves workers and the people who go through it? If they want to, they don't have to. I believe that all people have a desire for certain clips of meaning and salience to what they do on a day-to-day basis. My worry with what you describe is it might not feel like it has meaning. I think there is some giant class of activity in the world that people do and that we pay for meaning. But I don't know what the best way to get there is to take some class of work and say this is work that we're protecting with a meaningful spring. Because I'm not confident that people in jobs naturally feel they're in need of a spring. Now, of course academic jobs are not meaningful. The research people do is shiny lines, paper, teachers. People take great pride in their research. It's meaningless. We solved that problem already. We're just going to take the academic model and instead of doing research, bring it to say half of the state. Just to keep it going. But isn't there a kind of angst and nihilism that can move in by achieving parts of academia for this reason? It seems like when you speak to people, even very smart people who sometimes are doing the things you describe, they know they could be doing different things and they're trapped into some kind of status game. Some of that. But even the Nobel laureates, their lives are different than each other. It can be very bitchy. That's just human nature, right? If the Nobel laureates aren't happy, there's no hope in the AI world. It's going to be much better than the world of the Nobel laureates. I think that all of this could happen sufficiently quickly that we might have the opportunity to just play different, higher status games that are afforded to us by AI and productive capacity of robots. I'm wondering if there are definitely going to be entirely new jobs that involve marshalling, fueling, AI systems, all kinds of work. Those are hard jobs. Yes, but I think that there are going to be analogs which look more like creative, fun exercises in getting AIs to do things or make things or almost carry out competitions and games where people can play them in one go. I think there will be entirely new forms of entertainment that has some amount of meaning and perhaps an economic engine where an entertainment professional can participate. I believe we're not that far from the age of AI candidates. I don't know how to do this in a safe way. What percentage of parents now will buy those teddy bears for their kids and grandkids? I mean, I've had this thought since I have a person. My child is almost two. And I am annoyed I can't buy them teddy bears. But you're an outlier. I don't know. You are co-founder of Rabbit Drop. I don't think I'm an outlier. I think that once"
}