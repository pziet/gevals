{
  "text": "Hello everyone, and welcome back to Conversations with Tyrone. Today I am at Anthropic with Jack Clark. As you may know, there is now an Anthropic Economic Index, which is measuring the effect of advanced AI on the U.S. economy. There are two associated reports, as of March 2025, and soon more to come. Jack, of course, is the co-founder of Anthropic. Before that, he was the policy director at OpenAI, a reporter at Bloomberg, and originally has his background in humanities, and he comes from Brightening. Jack, welcome. Thanks for having me, Tyrone. Pleasure to be here. Where is it in our economy that AGI will affect the mass in a significant manner? Ooh, I'd hazard a guess that it's going to be things that are the trades and the most artisanal parts of them. So you might think of trades as having things like electricians or plumbing or also things like gardening, but I think within those you get certain high-status, high-skilled parts where people want to use a certain tradesman, not just because of their skill, but because of their notoriety and sometimes an aesthetic quality. I think that my take might be gardening, actually. And they won't use AGI to help design the garden, or just the human front will never disappear? I think the human front will never disappear, and people will purchase certain things because of the taste of the person, even if that taste looks like certain types of modern art production where the artist actually backs onto thousands of people that work for them, and they're more orchestrating it. And how about in the more desk-bound part of the service sector? Where will it come last? Come last? Ooh, good question. I think that on this, there are certain types of desk-bound work that just require talking to other people and getting to alignment or agreement. You might, if you count certain types of sales... It's great if you need that already, right? It's a wonderful paraphrase. It is, but we don't send Claude to sell Claude. Yet we send people to sell Claude, even though Claude could probably generate the text to do the sales motion. People want to do commerce with other people. So I think that there'll be certain relationships which get mediated by people, and people will have a strong preference probably for deals that they make on behalf of their larger pools of capital, where the deals are done by human proxies for large automated organizations with pools of capital. Where will AGI encounter the strongest legalized use? I think a few years ago it was encountering quite strong opposition from the law itself, because lawyers tended to like being able to charge very high prices and had a dislike of things that could bid them down. But my less flippant answer is probably big chunks of healthcare, because it's bound up in certain things around how we handle personal data, all of the standards around that. All of those standards are probably going to need to be changed in some form to make them amenable to being used by AI. And we've had a really hard time updating or changing data standards in general. But once you can put the AI on your own hard drive, which will be pretty soon, right? Won't that all change? It will change and reform the gray market expertise, but not official expertise. I had a baby recently, and so whenever my baby bonks their head, while I'm dialing the advice nurse, I talk to Claude just to reassure myself that the baby isn't in trouble. I don't think we actually fully permit healthcare uses by our own terms of service. We don't recommend it because we're worried about all of the liability issues this contains. But I know from my revealed preference that I'm always going to want to use that. But I can't take that Claude assessment and give it to Kaiser Permanente. I actually have to talk through a human to get everything else to happen on the back end to work out if they need to prescribe my child something. So the number one job will be surreptitiously transmitting the generation of information that comes from AIs, in essence? Some of it may be that. Some of it is about laundering the information that comes from AIs into human systems that are not predisposed to that information going in directly. I was thinking you might say that the United States government or some parts of it would be where strong AI would come last. I think that would be my forecast. They still use software from the 60s or maybe even the 50s sometimes. They do, but I actually suspect that that could be a place where we see really rapid changes, actually. And for a couple of reasons. One, we know that AI has relevance to national security. It develops certain types of capabilities. Oh yeah, that will happen. That will happen. But the rest of government. The rest of government. Department of Education. The non-scary parts of government. Right. Yeah. I would wager that it will become surprisingly easy to get it into really hard parts of government and then there will be a question of political will. But all around the world, and I'm sure you experience this, governments desperately want growth and they desperately want efficiency, and you see that here today. They say that. Oh, I agree there. I think if you look at also things like voter polling and other things, people want to see more changes out of government than they're currently getting. And I think sometimes constituent preferences do ultimately change what their elected officials do. I would just take the other side of this, but government may move slightly faster than you think. It may be very large established companies that end up having some of the greatest resistance to this in certain areas. Let's say it was decided that half of the staff of HUD or Department of Education could be replaced by strong AI or AGI. Do we first need to hire more people to make that happen? And who is it we can hire at what wage that switches the system so you can lay off the remaining half? I don't understand how that's ever going to work. I think that it will only work in a scenario where the system has got so powerful that you can bring the AI system in itself to help you think through this. And then there will be a question of political will, which is where the system might, this all might break down. What do you think is the chance that we decide to protect, say, half of the jobs in existence today with laws analogous to those we find in law and medicine against strong AGI? I think there is a high chance for a political movement to arrive which tries to freeze a load of human jobs in bureaucratic amber as a way to deal with the political issues posed by this incredibly powerful technology and how fast the changes are going to come from. I don't think that we'll do this in a reasoned way. I think it will be driven by the chaotic winds of political forces. And it feels like the sort of outcome that happens if we, as the companies building this stuff and our customers, don't generate enough examples of what good transitions look like. The fewer bits of evidence you have there and the more evidence you have of larger economic changes, probably the higher chance there will be a desire to step in and protect workers in different domains. But isn't it possible that it's actually, in a constrained sense, the very best outcome? People will still have jobs, they'll go somewhere in the morning. A lot of the work, especially the hard work, will be done by the AIs. Obviously we'll be richer so we can afford this all of a sudden. It won't feel that bad to people, life will look familiar. Isn't that, in a sense, what we should be aiming for? I think we should be aiming for... In the same sense, you might need, say, a generous welfare state to have free trade. The welfare state isn't always the most efficient, but if people accept free trade, it's an okay bargain. Isn't this, in a sense, the welfare state for the service workers? And they still get to go somewhere in the morning if they want to, but they don't have to. I believe that people, all people, have a desire for meaning and salience to what they do on a day-to-day basis. And my worry with what you describe is it might not feel like it has meaning, sufficient meaning. I think that there is some giant class of activity that we want to continue happening in the world that people do and from which they draw meaning. But I don't know that the best way to get there is to take some class of work and say, this is work that we're protecting and from which meaning will spring, because I'm not confident that you'll pick a load of jobs which naturally create their own meaning in that sense. But hasn't this succeeded in academia? So most academic jobs are not that meaningful. The research people do is not read by anyone, maybe the teachers. The people take great pride in their research, they put a lot of effort into it. It's meaningless. It seems we solved that problem already. We're just going to take the academic model, but instead of it being research, bring it to, say, half of our current economy, just to keep it going. But isn't there a kind of angst and nihilism even within high-achieving parts of academia for this reason? It seems like when you speak to people, even very smart people, who sometimes are doing the things you describe, they know they could be doing different things and they're trapped into some kind of status quo. There's some of that, but even the Nobel laureates, they're rivalrous with each other, they can be very bitchy, very petty, but that's just human nature, right? If the Nobel laureates aren't happy, there's no post-AI world that's going to do much better than how we're doing for the Nobel laureates today, right? Yeah, maybe my pushback is, I think that all of this could happen sufficiently quickly that we might have the opportunity to just play different higher-status games that are afforded to us"
}